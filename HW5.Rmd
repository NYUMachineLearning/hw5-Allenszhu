---
title: 'Machine Learning 2019: Tree-Based Methods'
author: "Allen Zhu"
date: "10/28/2019"
output:
  html_document:
    df_print: paged
  pdf: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tree-Based Methods 

Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems. Tree-based methods works for both categorical and continuous input and output variables.

```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(ISLR)
library(tree)
library(randomForest)
library(MASS)
library(gbm)
library(rpart)
library(rpart.plot)
```


## Homework

1. Attempt a regression tree-based method (not covered in this tutorial) on a reasonable dataset of your choice. Explain the results. 

```{r}
library(mlbench)
data(BostonHousing)
head(BostonHousing)
BH = BostonHousing
```

CP stands for the complexity parameter and represents the benefit of adding another layer to the tree. In the table generated below, it can be observed that by the addition of the first layer increases the predictive capability of the model by 45%, and a second layer further raises its predictive probability by an additional 17%. The default cut off in R for the cp value is 0.01, and that represents the level where additional levels provide minimal benefits to improving the model.

X-error represents is another metric that can be used to measure the benefits of adding layers to the tree. X-error is the cross validation error rate, where additional layers lower the error rate. As can be observed below, additional samples decreases the error rate farily significantly until it reaches the 5th layer, where a 0.0002 decrease is observed. At that point, the rate decrease is fairly small, and additional layers will run the risk of overfitting. 

```{r}

fit <- rpart(medv~.,method="anova", data=BH)


printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results



```
The below shows two plots. The first one plots the cross validation results against the actual results, whereas the second graph plots the cross validation results as a response to adding splits to the tree. 

```{r}
par(mfrow=c(1,2)) # two plots on one page
rsq.rpart(fit) # visualize cross-validation results   

```

Here I plotted the regression tree twice, once using the generic plot function, and another with the rpart plot function. The tree itself corresponds correctly with the cp and cross validation. Looking at the plotcp graph, it looks like the cut off is at around 7 splits, and that's how many splits are present in the tree below.

```{r}
plot(fit, uniform=TRUE,
   main="Regression Boston Housing Diagnosis ")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

rpart.plot(fit)

```


2. Attempt both a bagging and boosting method on a reasonable dataset of your choice. Explain the results.

```{r Random Forest HW}
#set seed for reproducibility 
set.seed(30)

#split into train and test sets (300 and 206 respectively)
train = sample(1:nrow(BH), 300)

#fit training subset of data to model 
rf.bh = randomForest(medv~., data = BH, subset = train)
rf.bh

#summary of rf.boston gives information about the number of trees, the mean squared residuals (MSR), and the percentage of variance explained

#No. of variables tried at each split: 4 
#Each time the tree comes to split a node, 4 variables would be selected at random, then the split would be confined to 1 of those 4 variables.

##Lets try a range of mtry (number of variables selected at random at each split)

oob.err = double(13)
test.err = double(13)

#In a loop of mtry from 1 to 13, you first fit the randomForest to the train dataset
for(mtry in 1:13){
  fit = randomForest(medv~., data = BH, subset=train, mtry=mtry, ntree = 350)
  oob.err[mtry] = fit$mse[350] ##extract Mean-squared-error 
  pred = predict(fit, BH[-train,]) #predict on test dataset
  test.err[mtry] = with(BH[-train,], mean( (medv-pred)^2 )) #compute test error
}

#Visualize 
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
```


At a glance, it looks like the model does quite well with the OOB samples where the MSE values hovering at around 5-10. The tree performs worse for the test data were the MSE values now hoever at around 15. This suggests that the model might be overfitted to the training data, where it may perform admirably on the training set, it becomes less capable of making accurate predictions on the testing data. 



```{r Boosting HW}

#Gradient Boosting Model
boost.BH = gbm(rm~., data = BH[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

#Variable Importance Plot
summary(boost.BH)


#Visualize important variables of interest
plot(boost.BH,i="dis")
plot(boost.BH,i="lstat")

#Predict on test set
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.BH, newdata = BH[-train,], n.trees = n.trees)
dim(predmat)

#Visualize Boosting Error Plot
boost.err = with(BH[-train,], apply( (predmat - rm)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")

```
The boosting summary showed that lstat and dis were the two most imformative variables in predicting the amount of rooms per dwelling. Looking online lstat stands for the lower status of the population, and medv stands for the median value of the houes. The two prediction maps also makes some intuitive sense. Houses have a higher value normally corresponds to more area - leading to more average rooms. The second prediction plot shows that as the lower status of the population increases, the number of average rooms decrease. This could be due to the fact that lower status of the population may tend to cluster around the city - an area where the average dwellings are apartments, and therefore have less rooms than the houses in the suburbs. 

The model was already hinted at in the random forest diagram to be overfitted, and the boosting test error graph here confirms it. Normally there is a slight dip in this graph to showcase the optimal amount of trees required to make a prediction, however, in this instance the graph never displays this dip. This suggests that the model is overfitted, and may require a dimension reduction to lower the overall complexity. A look at the relative influence table suggests that rad, ptratio, and taxes would be good variables to drop when considering the number of rooms a house has. None of those variables break a relative influence value of 2.
